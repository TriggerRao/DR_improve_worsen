{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cfHu9ewe2lrq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import VGG16, ResNet152\n",
        "# from tensorflow.keras.applications import DenseNet121,DenseNet201,EfficientNetB4,InceptionV3,ResNet152,VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from keras.backend import clear_session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZv4SPJtnO-6",
        "outputId": "d7b0cf91-0fc4-4f8c-bca8-85b8b4cd0422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Progression_DR/Improvement_Worsening')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F3q-5jHS1sJz"
      },
      "outputs": [],
      "source": [
        "image_paths = []\n",
        "labels = []\n",
        "file_paths = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6-Y7iWjfVdsf"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('DR_progress_data_final_Worsen_aug.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hHTKPguyTvls"
      },
      "outputs": [],
      "source": [
        "all_relevant_rows = []\n",
        "# Iterate over directories and files\n",
        "for i in range(-1, 2):\n",
        "    directory_path = os.path.join('DR_Worsening_aug_final', str(i))\n",
        "\n",
        "    # Use a list comprehension to get the image files in the directory\n",
        "    image_files = [file_ for file_ in os.listdir(directory_path) if file_.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    # Use a list comprehension to get the file IDs from the image files\n",
        "    file_ids = [file_.split('.')[0] for file_ in image_files]\n",
        "\n",
        "    # Filter the DataFrame to get the rows corresponding to the file IDs\n",
        "    relevant_rows = df[df['ID1'].isin(file_ids)]\n",
        "\n",
        "    for file_ in image_files:\n",
        "        file_id = file_.split('.')[0]\n",
        "\n",
        "        # Retrieve the label for the current file_id\n",
        "        filtered_rows = relevant_rows.loc[relevant_rows['ID1'] == file_id]\n",
        "        label = filtered_rows['y_od_os'].values\n",
        "\n",
        "        if label.size > 0:\n",
        "            labels.append(label[0])\n",
        "            all_relevant_rows.append(filtered_rows.iloc[0])\n",
        "            file_path = os.path.join(directory_path, file_)\n",
        "            image_paths.append(file_path)\n",
        "        else:\n",
        "            print(f'No label found for file ID: {file_id}')\n",
        "\n",
        "# Verify the lengths match\n",
        "assert len(image_paths) == len(labels), \"Mismatch between image paths and labels\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDT_w9vVXmaz",
        "outputId": "ce279e49-8c5c-462e-88a7-5938631cecbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n",
            "30000\n"
          ]
        }
      ],
      "source": [
        "print(len(labels))\n",
        "print(len(image_paths))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure train_labels are one-hot encoded\n",
        "labels = to_categorical(labels, num_classes=3, dtype='uint8')\n",
        "print(labels[:10])\n",
        "print(len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkavY1_P2cS1",
        "outputId": "201f1bdf-3010-441a-ca9f-cd6fed8c6fe6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]]\n",
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RG9uV-8QgHI4",
        "outputId": "d714b3a2-92e2-4666-9bdc-53e5999b7457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21000\n",
            "21000\n",
            "len(val_paths) 5250\n",
            "len(val_labels) 5250\n"
          ]
        }
      ],
      "source": [
        "# First split: train+validation and test\n",
        "train_val_paths, test_image_paths, _, test_rows, train_val_labels, test_labels = train_test_split(image_paths, all_relevant_rows, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Second split: train and validation\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(train_val_paths, train_val_labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# train_image_paths, test_image_paths, _, test_rows, train_labels, test_labels = train_test_split(image_paths, all_relevant_rows, labels, test_size=0.3, random_state=42)\n",
        "print(len(train_val_paths))\n",
        "print(len(train_val_labels))\n",
        "print(\"len(val_paths)\", len(val_paths))\n",
        "print(\"len(val_labels)\", len(val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = pd.DataFrame(columns = df.columns)\n",
        "relevant_rows_df = pd.DataFrame(test_rows)\n",
        "new_df = pd.concat([new_df, relevant_rows_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "qE925K5kaCb4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('.'.join(test_image_paths[0].split('/')[-1].split('.')[:-1]))\n",
        "new_df['predictions'] = ''\n",
        "new_df.to_csv('predicted_results.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "C_6_dR2OCvW1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "H9wf5M0aK5oY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "import cv2\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, image_paths, labels, batch_size=32, target_size=(256, 256), rescale=1./255, num_classes = 3, shuffle=True):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.rescale = rescale\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.indices = np.arange(len(self.image_paths))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X, y = self.__data_generation(batch_indices)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def __data_generation(self, batch_indices):\n",
        "        X = np.empty((len(batch_indices), *self.target_size, 3), dtype=np.float32)\n",
        "\n",
        "        y = np.empty((len(batch_indices), self.num_classes), dtype=np.float32)  # Modify this according to how your labels are structured\n",
        "\n",
        "        for i, idx in enumerate(batch_indices):\n",
        "            image = cv2.imread(self.image_paths[idx])\n",
        "            image = cv2.resize(image, self.target_size)\n",
        "            image = image.astype('float32') * self.rescale\n",
        "            X[i, ] = image\n",
        "            y[i] = self.labels[idx]\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REEqAQpFxOZ_",
        "outputId": "93b8467b-c1e4-4d1b-edcf-22244889464c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 78/493 [===>..........................] - ETA: 1:11:33 - loss: 1.1692 - accuracy: 0.3486"
          ]
        }
      ],
      "source": [
        "num_folds = 3\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "clear_session()\n",
        "\n",
        "# base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Optionally, unfreeze the last few layers for fine-tuning\n",
        "for layer in base_model.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# # Add the top layers\n",
        "model = Sequential()\n",
        "model.add(base_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model for classification\n",
        "model.compile(optimizer=Adam(learning_rate=0.00001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.000001)\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_filepath = os.path.join('./checkpoints', f'vgg16_fold_final_checkpoint.keras')\n",
        "checkpoint = ModelCheckpoint(\n",
        "    checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=8,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Create data generators for this fold\n",
        "train_generator = DataGenerator(train_paths, train_labels, batch_size=32, target_size=(256, 256), rescale=1./255)\n",
        "val_generator = DataGenerator(val_paths, val_labels, batch_size=32, target_size=(256, 256), rescale=1./255, shuffle=False)\n",
        "\n",
        "# Train the model on the current fold\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=1,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr],\n",
        "    batch_size=32,\n",
        "    )\n",
        "\n",
        "# Calculate training loss and accuracy for the current fold\n",
        "train_loss, train_accuracy = model.evaluate(train_generator, verbose=1)\n",
        "\n",
        "# Calculate validation loss and accuracy for the current fold\n",
        "val_loss, val_accuracy = model.evaluate(val_generator, verbose=1)\n",
        "\n",
        "model.save(\"checkpoints/vgg16_final.keras\")\n",
        "\n",
        "print(f\"\\nAverage Training Loss: {train_loss}\")\n",
        "print(f\"Average Training Accuracy: {train_accuracy}\")\n",
        "\n",
        "print(f\"\\nAverage Validation Loss: {val_loss}\")\n",
        "print(f\"Average Validation Accuracy: {val_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-fCVNTdrXbt_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "f250c366-70d4-496f-e14e-83c81c7b6f35"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'keras' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-98f9739e8060>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/vgg16_final.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create a data generator for the test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ],
      "source": [
        "# prompt: Load and test the saved model using test_image_paths\n",
        "predicted_df = pd.read_csv('predicted_results.csv')\n",
        "predicted_df.columns\n",
        "\n",
        "# Load the saved model\n",
        "model = keras.models.load_model('checkpoints/vgg16_final.keras')\n",
        "\n",
        "# Create a data generator for the test images\n",
        "test_generator = DataGenerator(test_image_paths, test_labels, batch_size=32, target_size=(256, 256), rescale=1./255, shuffle=False)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
        "\n",
        "# Print the results\n",
        "print(f\"\\nAverage Test Loss: {test_loss}\")\n",
        "print(f\"Average Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Make predictions on the test data\n",
        "test_predictions = model.predict(test_generator)\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "# predicted_ages = []\n",
        "for i, test_image_path in enumerate(test_image_paths):\n",
        "    file_id = test_image_path.split('/')[-1].split('.')[0]\n",
        "    predicted_age = test_predictions[i][0]\n",
        "    predicted_df.loc[df['ID1'] == file_id, 'predicted_age'] = predicted_age\n",
        "    # predicted_ages.append({'ID1': file_id, 'predicted_age': predicted_age})\n",
        "\n",
        "predicted_df.to_csv('predicted_results_1.csv', sep=',', encoding='utf-8', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d0FDjqkU5KcK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}